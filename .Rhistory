sampled$class
library(cluster)
set.seed(235)
clus <- kmeans(sampled[,1:50], 3)
clustered <- factor(clus$cluster)
sampled$class
table(clustered, )
clustered
sampled$class
levels(sampled$class)
levels(sampled$class) <- c("2,3,1")
levels(sampled$class) <- c("2","3","1")
sampled$class
table(clustered, sampled$class)
levels(sampled$class) <- c("1","2","3")
table(clustered, sampled$class)
sampled$class
levels(sampled$class)
sampled$class
class <- c(rep(1,20), rep(2,20),rep(3,20))
table(clustered, class)
clus4 <- kmeans(sampled[,1:50, 2])
clus4$cluster
clus4 <- kmeans(sampled[,1:50],2)
clus4$cluster
clust5 <- kmeans(sampled[,1:50],4)
clust5$cluster
pr.out$x
pr.out$x[1:2]
pr.out$x[,1:2]
pr1.2 <- pr.out$x[,1:2]
set.seed(123)
clus6 <- kmeans(pr1.2, 3)
clus6$cluster
set.seed(1234)
clus6 <- kmeans(pr1.2, 3)
clus6$cluster
set.seed(12345)
clus6 <- kmeans(pr1.2, 3)
clus6$cluster
set.seed(0)
clus6 <- kmeans(pr1.2, 3)
clus6$cluster
set.seed(01)
clus6 <- kmeans(pr1.2, 3)
clus6$cluster
set.seed(012)
clus6 <- kmeans(pr1.2, 3)
clus6$cluster
set.seed(0122)
clus6 <- kmeans(pr1.2, 3)
clus6$cluster
set.seed(01122)
clus6 <- kmeans(pr1.2, 3)
clus6$cluster
table(clus6$cluster,class)
scaled_data <- scale(sampled[,1:50])
set.seed(123)
clus7 <- kmeans(scaled_data,3)
clus7$cluster
set.seed(1234)
clus7 <- kmeans(scaled_data,3)
clus7$cluster
set.seed(12341)
clus7 <- kmeans(scaled_data,3)
clus7$cluster
set.seed(123411)
clus7 <- kmeans(scaled_data,3)
clus7$cluster
set.seed(12411)
clus7 <- kmeans(scaled_data,3)
clus7$cluster
set.seed(1411)
clus7 <- kmeans(scaled_data,3)
clus7$cluster
set.seed(411)
clus7 <- kmeans(scaled_data,3)
clus7$cluster
set.seed(4111)
clus7 <- kmeans(scaled_data,3)
clus7$cluster
set.seed(11)
clus7 <- kmeans(scaled_data,3)
clus7$cluster
set.seed(110)
clus7 <- kmeans(scaled_data,3)
clus7$cluster
set.seed(1130)
clus7 <- kmeans(scaled_data,3)
clus7$cluster
set.seed(130)
clus7 <- kmeans(scaled_data,3)
clus7$cluster
set.seed(0)
clus7 <- kmeans(scaled_data,3)
clus7$cluster
set.seed(0123)
clus7 <- kmeans(scaled_data,3)
clus7$cluster
set.seed(04123)
clus7 <- kmeans(scaled_data,3)
clus7$cluster
set.seed(064123)
clus7 <- kmeans(scaled_data,3)
clus7$cluster
set.seed(04123)
clus7 <- kmeans(scaled_data,3)
clus7$cluster
set.seed(0434123)
clus7 <- kmeans(scaled_data,3)
clus7$cluster
set.seed(04134123)
clus7 <- kmeans(scaled_data,3)
clus7$cluster
set.seed(044134123)
clus7 <- kmeans(scaled_data,3)
clus7$cluster
scaled_data <- scale(sampled[,1:50])
set.seed(044134123)
clus7 <- kmeans(scaled_data,3)
clus7$cluster
set.seed(6044134123)
clus7 <- kmeans(scaled_data,3)
clus7$cluster
set.seed(60441314123)
clus7 <- kmeans(scaled_data,3)
clus7$cluster
set.seed(123)
clus7 <- kmeans(scaled_data,3)
clus7$cluster
set.seed(1253)
clus7 <- kmeans(scaled_data,3)
clus7$cluster
set.seed(953)
clus7 <- kmeans(scaled_data,3)
clus7$cluster
set.seed(9530)
clus7 <- kmeans(scaled_data,3)
clus7$cluster
table(clus7$cluster, class)
data(USArrest)
data(USArrests)
force(USArrests)
USArrests
arrest <- na.omit(USArrests)
arrest2 <- data.frame(arrest[,1:4])
hclust(dist(arrest), method = "complete")
hc.arrest <- hclust(dist(arrest), method = "complete")
plot(hc.arrest)
hc.arrest
hc.arrest$height
hc.arrest$labels
cutree(hc.arrest, k=3, h = 102.86)
arrest
arrest2 <- scale(arrest)
hc.arrest2 <- hclust(dist(arrest2), method = "complete")
plot(hc.arrest2)
olives <- read_csv("Olives.csv")
olives
olives <- olives[,-1]
olives
olives <- olives[,-2]
olives$region <- factor(olives$region)
olives
#using kmeans first
kclus <- kmeans(olives[,-1],3)
#hierarchal
hclus <- hclust(dist(olives), method = "average")
plot(olives[,-1], col = (kclus$cluster))
plot(olives[,-1], col = (kclus$cluster+1))
hclus
summary(kclus)
#plots and summary for hclus
plot(hclus)
summary(hclus)
pc.olive <- prcomp(olives[,-1])
pc.olive
summary(pc.olive)
pc.olive
summary(pc.olive)
pc.olive
pc.olive$x
pc.olive$x[,1:2]
cbind(pc.olive$x[,1:2], olives$region)
olive_df <- cbind(pc.olive$x[,1:2], olives$region)
plot(x = olive_df[,1], y = olive_df[,2], col = olive_df[,3])
#doing k means on PCA1 and PCA2
olive_k <- pc.olive$x[,1:2]
o_clus <- kmeans(olive_k, 3)
o_clus <- kmeans(olive_k, 3)$cluster
olive_clustered <- cbind(olive_k, o_clus)
olive_clustered
plot(x = olive_clustered[,1], y = olive_clustered[,2], col = olive_clustered[,3])
plot(x = olive_df[,1], y = olive_df[,2], col = olive_df[,3])
plot(x = olive_clustered[,1], y = olive_clustered[,2], col = olive_clustered[,3])
plot(x = olive_df[,1], y = olive_df[,2], col = olive_df[,3])
plot(x = olive_clustered[,1], y = olive_clustered[,2], col = olive_clustered[,3])
plot(x = olive_df[,1], y = olive_df[,2], col = olive_df[,3])
plot(x = olive_clustered[,1], y = olive_clustered[,2], col = olive_clustered[,3])
#doing k means on PCA1 and PCA2
olive_k <- pc.olive$x[,1:2]
o_clus <- kmeans(olive_k, 3)$cluster
olive_clustered <- cbind(olive_k, o_clus)
plot(x = olive_clustered[,1], y = olive_clustered[,2], col = olive_clustered[,3])
plot(x = olive_df[,1], y = olive_df[,2], col = olive_df[,3])
plot(x = olive_clustered[,1], y = olive_clustered[,2], col = olive_clustered[,3])
plot(x = olive_df[,1], y = olive_df[,2], col = olive_df[,3])
plot(x = olive_clustered[,1], y = olive_clustered[,2], col = olive_clustered[,3])
plot(x = olive_df[,1], y = olive_df[,2], col = olive_df[,3])
plot(x = olive_clustered[,1], y = olive_clustered[,2], col = olive_clustered[,3])
plot(x = olive_df[,1], y = olive_df[,2], col = olive_df[,3])
plot(x = olive_clustered[,1], y = olive_clustered[,2], col = olive_clustered[,3])
setwd("~/Desktop/SCHOOL WORK/Senior/Fall 2020/Stats 101C/Kaggle")
# Chunk 1
library(tidyverse)
library(tidymodels)
library(vip)
airlines <- read_csv("airlines.csv") #showing which airlines each are in
samp <- read_csv("FlightSampleSubNew1.csv") #sample of how the submission should look like
testNoY <- read_csv("FlightTestNoYNew1.csv") #testing the data which we will use for predictions
flight <- read_csv("FlightTrainNew1.csv") #our training data
# Chunk 1
library(tidyverse)
library(tidymodels)
library(vip)
airlines <- read_csv("airlines.csv") #showing which airlines each are in
samp <- read_csv("FlightSampleSubNew1.csv") #sample of how the submission should look like
testNoY <- read_csv("FlightTestNoYNew1.csv") #testing the data which we will use for predictions
flight <- read_csv("FlightTrainNew1.csv") #our training data
View(flight)
# Chunk 1
library(tidyverse)
library(tidymodels)
library(vip)
airlines <- read_csv("airlines.csv") #showing which airlines each are in
samp <- read_csv("FlightSampleSubNew1.csv") #sample of how the submission should look like
testNoY <- read_csv("FlightTestNoYNew1.csv") #testing the data which we will use for predictions
flight <- read_csv("FlightTrainNew1.csv") #our training data
# Chunk 2
#changing character into factors
flight <- flight %>%
mutate_if(is.character, factor)
#variables that i'll be using
flight_df <- flight %>%
select(Org_airport_lat, Org_airport_long, Dest_airport_lat, Dest_airport_long, SCHEDULED_DEPARTURE, SCHEDULED_TIME, SCHEDULED_ARRIVAL,MONTH, DAY, DIVERTED, Cancelled)
testNoY <- testNoY %>%
select(Org_airport_lat, Org_airport_long, Dest_airport_lat, Dest_airport_long, SCHEDULED_DEPARTURE, SCHEDULED_TIME, SCHEDULED_ARRIVAL,MONTH, DAY, DIVERTED)
testNoY$DIVERTED <- factor(testNoY$DIVERTED)
flight_df$DIVERTED <- factor(flight_df$DIVERTED)
flight_split <- initial_split(flight_df, prop = 0.7, strata = Cancelled)
flight_train <- training(flight_split)
flight_test <- testing(flight_split)
#cross validation folds
set.seed(2)
flight_folds <- vfold_cv(flight_train, strata = Cancelled,v=10)
flight_rec <- recipe(Cancelled ~ ., data = flight_train) %>%
step_dummy(DIVERTED,one_hot = TRUE)
rf_spec <- rand_forest(trees = 1000,
mtry = 6) %>%
set_mode("classification") %>%
set_engine("ranger")
rf_wf <- workflow() %>%
add_recipe(flight_rec) %>%
add_model(rf_spec)
doParallel::registerDoParallel()
rf_final <- rf_wf %>%
fit(flight_train)
workflow() %>%
add_recipe(flight_rec) %>%
add_model(rf_final)
rf_final %>%
pull_workflow_fit() %>%
vip(aesthetics = list(alpha = 0.8, fill = "midnightblue"))
rf_spec <- rand_forest(trees = 1000,
mtry = 6) %>%
set_mode("classification") %>%
set_engine("ranger", importance = "permutation")
rf_wf <- workflow() %>%
add_recipe(flight_rec) %>%
add_model(rf_spec)
doParallel::registerDoParallel()
rf_final <- rf_wf %>%
fit(flight_train)
rf_final %>%
pull_workflow_fit() %>%
vip(aesthetics = list(alpha = 0.8, fill = "midnightblue"))
setwd("~/Desktop/SCHOOL WORK/Senior/Fall 2020/Stats 140SL/Final_Project")
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2
library(tidyverse)
library(tidytext)
library(tidylo)
listings <- read_csv("listings_cleaned_2clus.csv")
listings <- listings[,-1]
listings$value <- factor(listings$value)
# Chunk 3
listings_unnested <-  listings %>%
unnest_tokens(word, description) %>%
anti_join(stop_words, by = "word") %>%
filter(!str_detect(word,"[^\x01-\x7F]"))
#looking at the distribution of the length of the descriptions
listings_unnested %>%
count(id, sort = TRUE) %>%
ggplot(aes(n)) +
geom_histogram()
listings_lo <- listings_unnested %>%
count(value, word) %>%
bind_log_odds(value, word,n) %>%
arrange(-log_odds_weighted)
listings_lo %>%
group_by(value) %>%
slice_max(log_odds_weighted, n = 10) %>%
ungroup() %>%
mutate(word = reorder(word, log_odds_weighted)) %>%
ggplot(aes(word, log_odds_weighted, fill = value)) +
geom_col(show.legend = FALSE) +
facet_wrap(~value, scales = "free") +
coord_flip() +
labs(x = "Words", y = "Log Weighted Odds")
library(tidymodels)
set.seed(123)
listings_split <- initial_split(listings, strata = value)
listings_train <- training(listings_split)
listings_test <- testing(listings_split)
library(themis)
library(textrecipes)
listings_rec <- recipe(value ~ description, data = listings_train) %>%
step_tokenize(description) %>%
step_stopwords(description) %>%
step_tokenfilter(description, max_tokens = 500) %>%
step_tfidf(description) %>%
step_normalize(all_predictors())
#creating a lasso model
lasso_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%
set_engine("glm")
lasso_wf <- workflow() %>%
add_recipe(listings_rec) %>%
add_model(lasso_spec)
lasso_fit <- lasso_wf %>%
fit(listings_train)
library(vip)
lasso_fit %>%
pull_workflow_fit() %>%
vi(lambda = best_auc$penalty) %>%
group_by(Sign) %>%
top_n(15, wt = abs(Importance)) %>%
ungroup() %>%
mutate(Importance = abs(Importance),
Variable = str_remove(Variable, "tfidf_description_"),
Variable = fct_reorder(Variable, Importance),
effect = ifelse(Sign == "NEG", "HIGHER", "LOWER")) %>%
ggplot(aes(x = Importance, y = Variable, fill = effect)) +
geom_col(show.legend = FALSE) +
facet_wrap(~effect, scales = "free_y")
lasso_test <- lasso_wf %>%
fit(listings_test)
lasso_pred <- predict(lasso_test, listings_test)
conf_mat <- table(prediction = lasso_pred$.pred_class, actual = listings_test$value); conf_mat
(conf_mat[1,1] + conf_mat[2,2]) / sum(conf_mat)
las_spec <- logistic_reg(penalty = tune(),mixture = 1) %>%
set_engine("glmnet")
las_wf <- workflow() %>%
add_recipe(listings_rec) %>%
add_model(las_spec)
lambda_grid <- grid_regular(penalty(), levels = 20)
lambda_grid
review_folds <- vfold_cv(listings_train, strata = value, v = 3)
review_folds
review_folds <- vfold_cv(listings_train, strata = value, v = 4)
review_folds
lambda_grid <- grid_regular(penalty(), levels = 10)
lambda_grid
review_folds <- vfold_cv(listings_train, strata = value, v = 5)
review_folds
lambda_grid <- grid_regular(penalty(), levels = 10)
set.seed(123)
review_folds <- vfold_cv(listings_train, strata = value, v = 5)
doParallel::registerDoParallel()
set.seed(5)
las_grid <- tune_grid(
las_wf,
resamples = review_folds,
grid = lambda_grid,
metrics = metric_set(accuracy)
)
lambda_grid <- grid_regular(penalty(), levels = 10)
set.seed(123)
review_folds <- vfold_cv(listings_train, strata = value, v = 5)
doParallel::registerDoParallel()
set.seed(5)
las_grid <- tune_grid(
las_wf,
resamples = review_folds,
grid = lambda_grid,
metrics = metric_set(accuracy)
)
best_tune <- las_grid %>%
select_best("accuracy")
best_tune
final_model <- finalize_workflow(las_wf, best_tune)
las_final_fit <- final_model %>%
fit(listings_train)
las_final_fit %>%
pull_workflow_fit() %>%
vi(lambda = best_auc$penalty) %>%
group_by(Sign) %>%
top_n(15, wt = abs(Importance)) %>%
ungroup() %>%
mutate(Importance = abs(Importance),
Variable = str_remove(Variable, "tfidf_description_"),
Variable = fct_reorder(Variable, Importance),
effect = ifelse(Sign == "NEG", "HIGHER", "LOWER")) %>%
ggplot(aes(x = Importance, y = Variable, fill = effect)) +
geom_col(show.legend = FALSE) +
facet_wrap(~effect, scales = "free_y")
las_final_fit %>%
pull_workflow_fit()
best_auc$penalty
best_tune
las_final_fit %>%
pull_workflow_fit() %>%
vi(lambda = best_tune$penalty) %>%
group_by(Sign) %>%
top_n(15, wt = abs(Importance)) %>%
ungroup() %>%
mutate(Importance = abs(Importance),
Variable = str_remove(Variable, "tfidf_description_"),
Variable = fct_reorder(Variable, Importance),
effect = ifelse(Sign == "NEG", "HIGHER", "LOWER")) %>%
ggplot(aes(x = Importance, y = Variable, fill = effect)) +
geom_col(show.legend = FALSE) +
facet_wrap(~effect, scales = "free_y")
las_test <- final_model %>%
fit(listings_test)
lasso_pred <- predict(lasso_test, listings_test)
lasso_pred <- predict(las_test, listings_test)
conf_mat <- table(prediction = lasso_pred$.pred_class, actual = listings_test$value); conf_mat
(conf_mat[1,1] + conf_mat[2,2]) / sum(conf_mat)
conf_mat
best_tune
listings_lo %>%
group_by(value) %>%
slice_max(log_odds_weighted, n = 10) %>%
ungroup() %>%
mutate(word = reorder(word, log_odds_weighted)) %>%
ggplot(aes(word, log_odds_weighted, fill = value)) +
geom_col(show.legend = FALSE) +
facet_wrap(~value, scales = "free") +
coord_flip() +
labs(x = "Words", y = "Log Weighted Odds")
?step_tfidf
head(juice(prep(listings_rec)))
lambda_grid <- grid_regular(penalty(), levels = 20)  #a grid with 20 different lambda values to try
set.seed(123)
lambda_grid <- grid_regular(penalty(), levels = 20)  #a grid with 20 different lambda values to try
set.seed(123)
review_folds <- vfold_cv(listings_train, strata = value, v = 5) #5 fold cross validation to test different lambda values
lasso_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%  #going to tune penalty
set_engine("glmnet")
lasso_wf <- workflow() %>%          #adding model and recipe to a workflow
add_recipe(listings_rec) %>%
add_model(lasso_spec)
lambda_grid <- grid_regular(penalty(), levels = 20)  #a grid with 20 different lambda values to try
set.seed(123)
review_folds <- vfold_cv(listings_train, strata = value, v = 5) #5 fold cross validation to test different lambda values
doParallel::registerDoParallel()  #speeds up the process
set.seed(5)
lasso_grid <- tune_grid(
las_wf,                        #going to tune the lasso model
resamples = review_folds,      #resampling with the cv folds made earlier
grid = lambda_grid,            #using the 20 different lambda values
metrics = metric_set(accuracy) #metric of success is going to be accuracy of predictions
)
best_tune <- lasso_grid %>%       #selecting best penalty value based on accuracy
select_best("accuracy")
final_model <- finalize_workflow(lasso_wf, best_tune)   #finalizing the model
lasso_fit <- final_model %>%    #fitting the training data to the model
fit(listings_train)
#observing which words in the description lead to higher prices
library(vip)
lasso_fit %>%
pull_workflow_fit() %>%
vi(lambda = best_tune$penalty) %>%
group_by(Sign) %>%
top_n(15, wt = abs(Importance)) %>%
ungroup() %>%
mutate(Importance = abs(Importance),
Variable = str_remove(Variable, "tfidf_description_"),
Variable = fct_reorder(Variable, Importance),
effect = ifelse(Sign == "NEG", "HIGHER", "LOWER")) %>%
ggplot(aes(x = Importance, y = Variable, fill = effect)) +
geom_col(show.legend = FALSE) +
facet_wrap(~effect, scales = "free_y")
lasso_fit %>%
pull_workflow_fit() %>%
vi(lambda = best_tune$penalty) %>%
group_by(Sign) %>%
top_n(10, wt = abs(Importance)) %>%
ungroup() %>%
mutate(Importance = abs(Importance),
Variable = str_remove(Variable, "tfidf_description_"),
Variable = fct_reorder(Variable, Importance),
effect = ifelse(Sign == "NEG", "HIGHER", "LOWER")) %>%
ggplot(aes(x = Importance, y = Variable, fill = effect)) +
geom_col(show.legend = FALSE) +
facet_wrap(~effect, scales = "free_y")
lasso_test <- final_model %>%
fit(listings_test)
lasso_pred <- predict(lasso_test, listings_test)
conf_mat <- table(prediction = lasso_pred$.pred_class, actual = listings_test$value); conf_mat
(conf_mat[1,1] + conf_mat[2,2]) / sum(conf_mat)
